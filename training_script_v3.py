# -*- coding: utf-8 -*-
"""Training_Script_V3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NnG6MbRCgP7r_r_I0Q2b5f_QwUDWddTP

# Requirements
"""

!pip install facenet_pytorch

from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import MultiStepLR
from torchvision import datasets, transforms
from torch import optim
import numpy as np
import warnings
import zipfile
import torch
import uuid
import time
import cv2
import os

warnings.filterwarnings('ignore')

"""# Collect Data"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!cp -r /content/drive/MyDrive/ArshaFace.zip /content/

"""# Image Collection"""

# root_dir = "/content/Train_Data"

# for i in range(300):
#   cap = cv2.VideoCapture(0)
#   print(f"Taking Picture number {i} ......")
#   time.sleep(5)
#   ret, frame = cap.read()
#   imgname = os.path.join(root_dir, f"_{i}.jpg")
#   cv2.imwrite(imgname, frame)
#   cv2.imshow('frame', frame)
#   time.sleep(3)

#   if cv2.waitKey(1) & 0xFF == ord('q'):
#     break

# cap.release()
# cv2.destroyAllWindows()

# from IPython.display import display, Javascript
# from google.colab.output import eval_js
# from base64 import b64decode

# # def take_photo(filename='/content/Train_Data/photo.jpg', quality=0.8):
#   # root = "/content/Train_Data"
#   # filename = input("Image named as ")
#   # filename = os.path.join(root, filename + ".jpg")

# def take_photo(filename, quality=0.8):
#   js = Javascript('''
#     async function takePhoto(quality) {
#       const div = document.createElement('div');
#       const capture = document.createElement('button');
#       capture.textContent = 'Capture';
#       div.appendChild(capture);

#       const video = document.createElement('video');
#       video.style.display = 'block';
#       const stream = await navigator.mediaDevices.getUserMedia({video: true});

#       document.body.appendChild(div);
#       div.appendChild(video);
#       video.srcObject = stream;
#       await video.play();

#       // Resize the output to fit the video element.
#       google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

#       // Wait for Capture to be clicked.
#       await new Promise((resolve) => capture.onclick = resolve);

#       const canvas = document.createElement('canvas');
#       canvas.width = video.videoWidth;
#       canvas.height = video.videoHeight;
#       canvas.getContext('2d').drawImage(video, 0, 0);
#       stream.getVideoTracks()[0].stop();
#       div.remove();
#       return canvas.toDataURL('image/jpeg', quality);
#     }
#     ''')
#   display(js)
#   data = eval_js('takePhoto({})'.format(quality))
#   binary = b64decode(data.split(',')[1])
#   with open(filename, 'wb') as f:
#     f.write(binary)
#   return filename

# from IPython.display import Image

# try:
#   for index in range(111,500):
#     print(f"Taking Picture number {index+1}/500 ......")
#     root = "/content/Train_Data"
#     filename = os.path.join(root, "Sample_" + str(index) + ".jpg")
#     path = take_photo(filename)
#     print('Saved to {}'.format(path))
#     print("-"*50)
#     time.sleep(2)

# except Exception as err:
#   print(str(err))

"""

> Here comes the LFW dataset

"""

!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz

!tar -xvzf lfw.tgz

# Unzip my own data
!unzip /content/ArshaFace.zip -d /content/

# Delete unnecessary directories (all of them expect Person 4)
!rm -r "/content/dataset/Person 14"

"""#### Make Sure to rename the data directory to this format:
- /content/dataset/Person1

"""

# Should be 90
os.rename("/content/dataset/Person 4", "/content/dataset/Person1")
len(os.listdir("/content/dataset/Person1"))

"""#### Now move all the LFW faces into another folder called Person2, as here we are only training the model to verify one face"""

from tqdm import tqdm
import shutil
import random

root_path = "/content/lfw"
dest_path = "/content/dataset/Person2"
os.makedirs(dest_path, exist_ok=True)

for person in tqdm(os.listdir(root_path)):
  person_path = os.path.join(root_path, person)
  print(f"{person} images are being moved ...")
  if len(os.listdir(root_path)) >= 1:
    sample = random.choice(os.listdir(person_path))
    src = os.path.join(person_path, sample)
    dest = os.path.join(dest_path, sample)
    shutil.move(src, dest)
    print("-"*100)

  else:
    print("No Images present in the folder!")

  # for image in os.listdir(person_path):
  #   src = os.path.join(person_path, image)
  #   dest = os.path.join(dest_path, image)
  #   shutil.move(src, dest)
  # print("-"*100)

# Should be about 5.7k
len(os.listdir("/content/dataset/Person1"))

"""# Data Augmentation"""

import Data_and_Label_Augmentation as DataGen

#### Initialize Image Custom Augmentation object
My_data = DataGen.Image_Custom_Augmentation(
                                    # SP_intensity=0.010,
                                    SP_intensity=False,
                                    CWRO_Key=20,
                                    CCWRO_Key=20,
                                    Br_intensity=25,
                                    H_Key = True,
                                    V_Key = False,
                                    HE_Key= False,
                                    GaussianBlur_KSize = 9,
                                    Random_Translation = True,
                                    Scaling_Range = (0.90, 1.10),
                                    Img_res=512
                                    )

#### Generate augmented data
My_data.Generate_Data(input_path="/content/dataset/Person1", output_path="/content/dataset/Person1")

"""# Fine-Tune"""

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print('Running on device: {}'.format(device))

"""

> Error Handling:

A hidden .ipynb file causes error, so make sure to run the cells bellow with respect to the directory of your dataset

"""

!rm -R /content/dataset/.ipynb_checkpoints

!rm -R /content/dataset/Person1/.ipynb_checkpoints

!rm -R /content/dataset/Person2/.ipynb_checkpoints

len(os.listdir("/content/dataset/Person2"))

data_dir = '/content/dataset'

batch_size = 1
epochs = 120
workers = 0 if os.name == 'nt' else 8

mtcnn = MTCNN(
    image_size=160, margin=0, min_face_size=20,
    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,
    device=device
)

dataset = datasets.ImageFolder(data_dir, transform=transforms.Resize((512, 512)))
# dataset = datasets.ImageFolder(data_dir)
dataset.samples = [
    (p, p.replace(data_dir, data_dir + '_cropped'))
        for p, _ in dataset.samples
]

loader = DataLoader(
    dataset,
    num_workers=workers,
    batch_size=batch_size,
    collate_fn=training.collate_pil
)

for i, (x, y) in enumerate(loader):
  try:
      mtcnn(x, save_path=y)
      print('\rimage number {} of {}'.format(i + 1, len(loader)), end='')

  except Exception as e:
    print(f"the problem is on x: {x}\n")
    print(f"the problem is on y: {y}")

# # Debugging the corrupted images

# valid_indices = []
# invalid_indices = []

# for i, (x, y) in tqdm(enumerate(loader)):
#     try:
#         # Process images with MTCNN
#         boxes, probs = mtcnn.detect(x)
#         # Filter out images where no face is detected and print the image name
#         for idx, box in enumerate(boxes):
#             if box is not None:
#                 valid_indices.append(dataset.samples[idx][0])
#             else:
#                 # print(f"\nNo face detected in image: {dataset.samples[idx][0]}")
#                 invalid_indices.append(dataset.samples[idx][0])

#     except Exception as e:
#         print(f"\nError in batch {i + 1}: {e}")
#         continue

# print(invalid_indices)

    #     x_valid = [x[i] for i in valid_indices]
    #     y_valid = [y[i] for i in valid_indices]

    #     if x_valid:
    #         mtcnn(x_valid, save_path=y_valid)

    #     print('\rBatch {} of {}'.format(i + 1, len(loader)), end='')

    # except Exception as e:
    #     print(f"\nError in batch {i + 1}: {e}")
    #     continue

# Remove mtcnn to reduce GPU memory usage
del mtcnn

# Just to check how many images the MTCNN failed to get
print(len(os.listdir("/content/dataset_cropped/Person1")))

resnet = InceptionResnetV1(
    classify=True,
    pretrained='vggface2',
    num_classes=len(dataset.class_to_idx)
).to(device)

!rm -R /content/dataset_cropped/.ipynb_checkpoints

!rm -R /content/dataset_cropped/Person1/.ipynb_checkpoints

loss_fn = torch.nn.CrossEntropyLoss()
# loss_fn = torch.nn.BCEWithLogitsLoss()
metrics = {
    'fps': training.BatchTimer(),
    'acc': training.accuracy
}

from torch.optim.lr_scheduler import ReduceLROnPlateau

# batch size
BS = 16
optimizer = optim.Adam(resnet.parameters(), lr=0.001)
scheduler = MultiStepLR(optimizer, [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110])
# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)


trans = transforms.Compose([
    np.float32,
    transforms.ToTensor(),
    fixed_image_standardization
])

dataset = datasets.ImageFolder("/content/dataset_cropped", transform=trans)
img_inds = np.arange(len(dataset))
np.random.shuffle(img_inds)
train_inds = img_inds[:int(0.75 * len(img_inds))]
val_inds = img_inds[int(0.75 * len(img_inds)):]

train_loader = DataLoader(
    dataset,
    num_workers=workers,
    batch_size=BS,
    sampler=SubsetRandomSampler(train_inds)
)
val_loader = DataLoader(
    dataset,
    num_workers=workers,
    batch_size=BS,
    sampler=SubsetRandomSampler(val_inds)
)

# def compute_accuracy(y_pred, y_true):
#     y_pred_binary = (y_pred > 0.5).float()
#     correct = (y_pred_binary == y_true).sum().item()
#     total = y_true.size(0)
#     accuracy = correct / total
#     return accuracy

# # Training loop
# for epoch in range(epochs):
#     print('\nEpoch {}/{}'.format(epoch + 1, epochs))

#     # Training loop
#     resnet.train()
#     train_loss = 0.0
#     train_correct = 0
#     train_total = 0

#     resnet.train()
#     for x, y in train_loader:
#         y = y.float().unsqueeze(1).to(device)
#         y_pred = resnet(x.to(device))
#         loss_batch = loss_fn(y_pred, y)

#         # Backpropagation and optimization
#         optimizer.zero_grad()
#         loss_batch.backward()
#         optimizer.step()

#         # Loss Report
#         train_loss += loss_batch.item() * x.size(0)
#         y_pred_binary = (y_pred > 0.5).float()
#         train_correct += (y_pred_binary == y).sum().item()
#         train_total += y.size(0)

#     train_loss /= train_total
#     train_accuracy = train_correct / train_total

#     print(f"Train Loss: {train_loss:.4f}")
#     print(f"Train Accuracy: {train_accuracy:.4f}")


#     # Validation Loop
#     resnet.eval()
#     val_loss = 0.0
#     val_correct = 0
#     val_total = 0

#     with torch.no_grad():
#       for x, y in val_loader:
#           y = y.float().unsqueeze(1).to(device)
#           y_pred = resnet(x.to(device))
#           loss_batch = loss_fn(y_pred, y)

#           val_loss += loss_batch.item() * x.size(0)
#           y_pred_binary = (y_pred > 0.5).float()
#           val_correct += (y_pred_binary == y).sum().item()
#           val_total += y.size(0)

#     val_loss /= val_total
#     val_accuracy = val_correct / val_total
#     print(f"Validation Loss: {val_loss:.4f}")
#     print(f"Validation Accuracy: {val_accuracy:.4f}")
#     print("="*100)

#     scheduler.step(val_loss)

# writer = SummaryWriter()
# writer.iteration, writer.interval = 0, 10

print('\n\nInitial')
print('-' * 10)
resnet.eval()
training.pass_epoch(
    resnet, loss_fn, val_loader,
    batch_metrics=metrics, show_running=True, device=device,
    # writer=writer
)

for epoch in range(epochs):
    print('\nEpoch {}/{}'.format(epoch + 1, epochs))
    print('-' * 10)

    resnet.train()
    training.pass_epoch(
        resnet, loss_fn, train_loader, optimizer, scheduler,
        batch_metrics=metrics, show_running=True, device=device,
        # writer=writer
    )

    resnet.eval()
    training.pass_epoch(
        resnet, loss_fn, val_loader,
        batch_metrics=metrics, show_running=True, device=device,
        # writer=writer
    )

# writer.close()

torch.save(resnet.state_dict(), '/content/Face_Verification_v4.pth')

!cp -r /content/Face_Verification_v4.pth /content/drive/MyDrive

# import re

# file = open("/content/viz.txt", 'r')
# for line in file.readlines():
#   if line.startswith("Train"):
#     a = re.search(r'\b(loss)\b', line)
#     print(f"loss index in text: {a.start()}")
#     b = re.search(r'\b(acc)\b', line)
#     print(f"accuracy index in text: {b.start()}")

# file.close()

# from tqdm import tqdm
# file = open("/content/viz.txt", 'r')

# train_loss = []
# train_acc = []

# for line in tqdm(file.readlines()):
#   if line.startswith("Train"):
#     # print(line[30:36]) # Train Loss Values
#     # print(line[64:70]) # Train Accuracy Values
#     train_loss.append(float(line[30:36]))
#     train_acc.append(float(line[64:70]))

# file.close()

# from tqdm import tqdm
# file = open("/content/viz.txt", 'r')

# val_loss = []
# val_acc = []

# for line in tqdm(file.readlines()):
#   if line.startswith("Valid"):
#     # print(line[30:36]) # Valid Loss Values
#     # print(line[64:70]) # Valid Accuracy Values
#     val_loss.append(float(line[30:36]))
#     val_acc.append(float(line[64:70]))

# file.close()

